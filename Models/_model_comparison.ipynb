{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pickle\nimport pandas as pd\nimport numpy as np\n# tokenization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# model\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\n# from keras.layers import Embedding\nfrom keras.models import Model,load_model\nfrom keras import layers\nfrom keras import Input\nfrom keras import optimizers\nfrom tensorflow.keras import regularizers\n# data split and result analysis\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve, auc, roc_curve\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nimport matplotlib.pyplot as plt\nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def test_model(logreg, X_test, y_test):\n    \n    y_pred = logreg.predict(X_test)\n    print(\"accuracy = %s\"%logreg.score(X_test, y_test))\n    print(\"f1 socre = %s\"%f1_score(y_test, y_pred))\n    print(\"recall__ = %s\"%recall_score(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"confusion matrix :\")\n    print(cm)\n    print(\"classification report :\")\n    print(classification_report(y_test,y_pred))\n    # PR curve\n    precision, recall = pr_curve_lr(logreg, X_test, y_test)\n    return precision, recall\n\ndef pr_curve_lr(logreg, X_test, y_test):\n    # predict probabilities\n    lr_probs = logreg.predict_proba(X_test)\n    # keep probabilities for the positive outcome only\n    lr_probs = lr_probs[:, 1]\n    # predict class values\n    y_hat = logreg.predict(X_test)\n    lr_precision, lr_recall, threds = precision_recall_curve(y_test, lr_probs)\n    lr_f1, lr_auc = f1_score(y_test, y_hat), auc(lr_recall, lr_precision)\n    # summarize scores\n    print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n    # plot the precision-recall curves\n    no_skill = len(y_test[y_test==1]) / len(y_test)\n    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n    plt.plot(lr_recall, lr_precision, marker='.', label='Logistic Regression')\n    # axis labels\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    return lr_precision, lr_recall\n\ndef extract_features(data):\n    '''\n    Output the feature lists that will be used to train model\n    Take the dataset as Input\n    '''\n    amazon_name = data[\"amazon_name\"].tolist()\n    google_name = data[\"google_name\"].tolist()\n    amazon_desc = data[\"amazon_description\"].tolist()\n    google_desc = data[\"google_description\"].tolist()\n    price_diff = np.asarray(data[\"price_diff\"])\n    price_indicator = np.asarray(data[\"price_nan_indicator\"])\n    \n    return amazon_name, google_name, amazon_desc, google_desc, price_diff, price_indicator\n\ndef tokenization (tokenizer, text, maxlen):\n    '''\n    Output the integer lists which meet the input requirement of Model\n    Take the tokenizer, text and considered max length as Input\n    '''\n    #convert to integer lists\n    sequences = tokenizer.texts_to_sequences(text)\n    # padding\n    sequences = pad_sequences(sequences, maxlen)\n    return sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_prob(model, X_test):\n    '''\n    Return probabilities for positive class calculated by sigmoid output layer\n    Take trained model and val or test data as Input\n    '''\n    prob_y = model.predict(X_test, verbose=0) \n    return prob_y        \n\ndef label_pred(prob_y, threshold):\n    '''\n    Return the labels predicted by sigmoid output lalyer under certain threshold\n    Take sigmoid output probability and threshold as Input\n    '''\n    pred_y = np.where(prob_y > threshold, 1, 0)\n    return pred_y\n \ndef c_matrix(model, X_test, y_test, threshold):\n    '''\n    Output confusion matrix\n    Take trained model, test data, test label and threshold as Input\n    '''\n    prob_y = model.predict(X_test, verbose=0) \n    pred_y = np.where(prob_y > threshold, 1, 0)\n    cm = confusion_matrix(y_test, pred_y)\n    print(cm)\n\ndef area_under_curve(model, X_test, y_test):\n    '''\n    Output AUC - Area Under Curve\n    Take trained model, test data and test label as Input\n    '''\n    y_probs = sigmoid_prob(model, X_test)\n    nn_precision, nn_recall, threds = precision_recall_curve(y_test, y_probs)   \n    nn_auc = auc(nn_recall, nn_precision)\n    return nn_auc, nn_recall, nn_precision\n\ndef pr_curve(model, X_test, y_test):\n    '''\n    PR curve and Area Under Curve\n    Take trained model, test data and test labels as Input\n    '''\n    nn_auc, nn_recall, nn_precision = area_under_curve(model, X_test, y_test)\n    print(\"The AUC is \",nn_auc)\n    no_skill = len(y_test[y_test==1]) / len(y_test)\n    \n    fig, axes = plt.subplots(1, 1, figsize=(6,6))\n    axes.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n    axes.plot(nn_recall, nn_precision, marker='.', label='DNN')\n    # axis labels\n    axes.set(xlabel='Recall')\n    axes.set(ylabel='Precision')\n    # show the legend\n    axes.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '../input/trained-models/LR_model.sav'\nlogreg = pickle.load(open(filename, 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load numeric test data\nnumeric_test = pd.read_csv(\"../input/numeric-dataset/numeric_test.csv\")\nprint(numeric_test[\"label\"].value_counts())\n\nX_test = numeric_test.iloc[:,0:4]\ny_test = numeric_test.iloc[:,4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_precision, lr_recall = test_model(logreg, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Siamese Neural Network","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/prepared-datasets/'\ntrain_set = pd.read_csv(path+'train_set.csv')\nval_set = pd.read_csv(path+'val_set.csv')\ntest_set = pd.read_csv(path+'test_set.csv')\n\ntrain_amazon_info = train_set[\"amazon_info\"].tolist()\ntrain_google_info = train_set[\"google_info\"].tolist()\nval_amazon_info = val_set[\"amazon_info\"].tolist()\nval_google_info = val_set[\"google_info\"].tolist()\n# Combine all text data\nall_data_text = train_amazon_info + train_google_info + val_amazon_info + val_google_info\n\n# Retrieve all the tokens in the dataset\ndef create_tokenizer(all_text):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(all_text)\n    word_index = tokenizer.word_index\n    print(\"Found %s unique tokens\"%len(word_index))\n    return tokenizer, word_index\n\n# Create tokenizer according to all the words in train data\ntokenizer, data_word_index = create_tokenizer(all_data_text)\n\n# Preprocess test data and labels\ntest_amazon_name, test_google_name, test_amazon_desc, test_google_desc, test_price_diff, test_price_indicator = extract_features(test_set)\n\nmaxlen = 200\ntest_amazon_name_seq = tokenization(tokenizer, test_amazon_name, maxlen)\ntest_google_name_seq = tokenization(tokenizer, test_google_name, maxlen)\ntest_amazon_desc_seq = tokenization(tokenizer, test_amazon_desc, maxlen)\ntest_google_desc_seq = tokenization(tokenizer, test_google_desc, maxlen)\n\ntest_data = [test_amazon_name_seq, test_google_name_seq, test_amazon_desc_seq, test_google_desc_seq, test_price_diff, test_price_indicator]\ntest_labels = np.asarray(test_set[\"label\"])\n\nprint(test_set[\"label\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\naverage_model = load_model('../input/trained-models/model_average.h5')\nlstm_model = load_model('../input/trained-models/model_LSTM.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c_matrix(average_model, test_data, test_labels, threshold=0.5)\nc_matrix(lstm_model, test_data, test_labels, threshold=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr_curve(average_model, test_data, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pr_curve(lstm_model, test_data, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid_prob(model, X_test):\n    '''\n    Return probabilities for positive class calculated by sigmoid output layer\n    Take trained model and val or test data as Input\n    '''\n    prob_y = model.predict(X_test, verbose=0) \n    return prob_y \n\ndef area_under_curve(model, X_test, y_test):\n    '''\n    Output AUC - Area Under Curve\n    Take trained model, test data and test label as Input\n    '''\n    y_probs = sigmoid_prob(model, X_test)\n    nn_precision, nn_recall, threds = precision_recall_curve(y_test, y_probs)   \n    nn_auc = auc(nn_recall, nn_precision)\n    return nn_auc, nn_recall, nn_precision\n\ndef label_pred(prob_y, threshold):\n    '''\n    Return the labels predicted by sigmoid output lalyer under certain threshold\n    Take sigmoid output probability and threshold as Input\n    '''\n    pred_y = np.where(prob_y > threshold, 1, 0)\n    return pred_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average 0.5 f1\nave_prob = sigmoid_prob(average_model, test_data)\nave_pred = label_pred(ave_prob, 0.5)\nave_f1 = f1_score(test_labels, ave_pred)\nprint(ave_f1)\n\n# LSTM 0.5 f1\nlst_prob = sigmoid_prob(lstm_model, test_data)\nlst_pred = label_pred(lst_prob, 0.5)\nlst_f1 = f1_score(test_labels, lst_pred)\nprint(lst_f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the probabilities given by LR\nlr_probs = logreg.predict_proba(X_test)\n# Keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\n# Get precisions and recalls\nlr_precision, lr_recall, threds = precision_recall_curve(y_test, lr_probs)\n# Get AUC\nlr_auc = auc(lr_recall, lr_precision)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ave_auc, ave_recall, ave_precision = area_under_curve(average_model, test_data, test_labels)\nlst_auc, lst_recall, lst_precision = area_under_curve(lstm_model, test_data, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ave_recall, ave_precision, marker='.', label=\"Siamese Average NN\")\nplt.plot(lst_recall, lst_precision, marker='.', label=\"Siamese LSTM NN\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision and Recall Curve ')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.8))\nplt.savefig(\"PR_Ave_LSTM.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lr_recall, lr_precision, marker='.', label=\"LR\")\nplt.plot(ave_recall, ave_precision, marker='.', label=\"SDNN\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision and Recall Curve ')\nplt.legend()\nplt.savefig(\"LR_Ave_PR.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_f1 = 2*lr_precision*lr_recall/(lr_precision+lr_recall)\nprint(\"LR Max f1: \",max(lr_f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ave_f1 = 2*ave_precision*ave_recall/(ave_precision+ave_recall)\nmax(ave_f1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_scores = 2*recall*precision/(recall+precision)\nprint('Best threshold: ', thresholds[np.argmax(f1_scores)])\nprint('Best F1-Score: ', np.max(f1_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(lr_recall, lr_precision, marker='.', label=\"Logistic Regression\")\nplt.plot(ave_recall, ave_precision, marker='.', label=\"Siamese Average NN\")\nplt.plot(lst_recall, lst_precision, marker='.', label=\"Siamese LSTM NN\")\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision and Recall Curve ')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.8))\nplt.savefig(\"PR_LR_Ave.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_loss_acc(history):\n    '''\n    Plot train val loss and acc\n    '''\n    fig, axes = plt.subplots(1, 2, figsize=(16,6))\n    axes[0].plot(history.history['loss'], label='loss')\n    axes[0].plot(history.history['val_loss'], label='val_loss')\n    axes[1].plot(history.history['acc'], label='acc')\n    axes[1].plot(history.history['val_acc'], label='val_acc')\n\n    for ax in axes:\n        ax.legend()\n        ax.grid(True)\n        ax.set(xlabel='epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nn_results = [[\"average\",0.365,0.313,0.776,0.196],[\"lstm\",0.37,0.347,0.293,0.425]]\n\ndf = pd.DataFrame(nn_results, columns=[\"method\",\"AUC\",\"F1\",\"Recall\",\"Precision\"])\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_bar = sns.barplot(x=\"method\", y=\"AUC\", data=df)\nrecall_bar.set(xlabel='', ylabel='Area Under Curve')\nplt.savefig(\"ave_lst_auc.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_bar = sns.barplot(x=\"method\", y=\"F1\", data=df)\nrecall_bar.set(xlabel='', ylabel='F1 Score')\nplt.savefig(\"ave_lst_f1.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_bar = sns.barplot(x=\"method\", y=\"Recall\", data=df)\nrecall_bar.set(xlabel='', ylabel='Recall')\nplt.savefig(\"ave_lst_recall.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_bar = sns.barplot(x=\"method\", y=\"Precision\", data=df)\nrecall_bar.set(xlabel='', ylabel='Precision')\nplt.savefig(\"ave_lst_precision.png\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}