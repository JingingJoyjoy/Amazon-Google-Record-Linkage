{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the packages required\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter","execution_count":95,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/amazon-google/\"\namazon_train = pd.read_csv(path+\"Amazon_train.csv\")\ngoogle_train = pd.read_csv(path+\"Google_train.csv\")\namazon_test = pd.read_csv(path+\"Amazon_test.csv\")\ngoogle_test = pd.read_csv(path+\"Google_test.csv\")\ntrain_perfect_matching = pd.read_csv(path+\"AG_perfect_matching_train.csv\")\ntest_perfect_matching = pd.read_csv(path+\"AG_perfect_matching_test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the first column of train-test datasets\namazon_train = amazon_train.drop(amazon_train.columns[0], axis=1)\ngoogle_train = google_train.drop(google_train.columns[0], axis=1)\namazon_test = amazon_test.drop(amazon_test.columns[0], axis=1)\ngoogle_test = google_test.drop(google_test.columns[0], axis=1)\n# Drop the first column of train-test matching datasets\ntrain_perfect_matching = train_perfect_matching.drop(train_perfect_matching.columns[0], axis=1)\ntest_perfect_matching = test_perfect_matching.drop(test_perfect_matching.columns[0], axis=1)\n\n# Unify the ID col_name (idAmazon & idGoogle)\namazon_train = amazon_train.rename(columns={\"id\":\"idAmazon\",\"title\": \"name\"})\ngoogle_train = google_train.rename(columns={\"id\":\"idGoogle\"})\namazon_test = amazon_test.rename(columns = {\"id\":\"idAmazon\",\"title\":\"name\"})\ngoogle_test = google_test.rename(columns={\"id\":\"idGoogle\"})\ntrain_perfect_matching = train_perfect_matching.rename(columns={\"idGoogleBase\":\"idGoogle\"})\ntest_perfect_matching = test_perfect_matching.rename(columns={\"idGoogleBase\":\"idGoogle\"})\n\n# The overview of the size of three training sets\nprint(\"No. of Samples in Amazon_Train: \"+str(len(amazon_train)))\nprint(\"No. of Samples in Google_Train: \"+str(len(google_train)))\nprint(\"No. of Matching Samples in AG_Train: \"+str(len(train_perfect_matching)))\nprint(\"\")\n\n# The overview of the size of three testing sets\nprint(\"No. of Samples in Amazon_Test: \"+str(len(amazon_test)))\nprint(\"No. of Samples in Google_Test: \"+str(len(google_test)))\nprint(\"No. of Matching Samples in AG_Test: \"+str(len(test_perfect_matching)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess text and numeric data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def text_preprocess(text):\n    # remove numbers and convert to lowercase\n    text_1 = (re.sub(r'\\d+','',str(text))).lower()\n    # replace NaN with blank\n    if(text_1 == \"nan\"):\n        return \" \"\n    # remove punctuation\n    text_2 = \"\".join([c for c in text_1 if c not in string.punctuation])\n    # remove multiple space\n    text_3 = re.sub(' +', ' ', text_2)\n    # remove Stopwords\n    text_tokens = word_tokenize(text_3)\n    text_4 = \" \".join([word for word in text_tokens if word not in stopwords.words('english')])\n#     # Lemmatization\n#     # initantiate lemmatizer\n#     lemmatizer = WordNetLemmatizer()\n#     text_5 = [lemmatizer.lemmatize(i) for i in text_4]\n#     # Stem\n#     # instantiate Stemmer\n#     stemmer = PorterStemmer()\n#     text_6 = \" \".join([stemmer.stem(j) for j in text_5])\n    return text_4.strip()\n\ndef num_preprocess(df_col):\n    if(df_col.dtype != \"float64\"):\n        # remove char in any\n        num_string = '0123456789.'\n        df_col = df_col.apply(lambda x: \"\".join([c for c in x if c in num_string]))\n        # change the data type to numeric\n        df_col = pd.to_numeric(df_col)\n#     # normalization\n#     df_col = (df_col - np.mean(df_col))/np.std(df_col)\n    return df_col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for text data\namazon_train[\"name\"] = amazon_train[\"name\"].apply(lambda x: text_preprocess(x))\namazon_train[\"description\"] = amazon_train[\"description\"].apply(lambda x: text_preprocess(x))\namazon_test[\"name\"] = amazon_test[\"name\"].apply(lambda x: text_preprocess(x))\namazon_test[\"description\"] = amazon_test[\"description\"].apply(lambda x: text_preprocess(x))\n\ngoogle_train[\"name\"] = google_train[\"name\"].apply(lambda x: text_preprocess(x))\ngoogle_train[\"description\"] = google_train[\"description\"].apply(lambda x: text_preprocess(x))\ngoogle_test[\"name\"] = google_test[\"name\"].apply(lambda x: text_preprocess(x))\ngoogle_test[\"description\"] = google_test[\"description\"].apply(lambda x: text_preprocess(x))\n\n# Join the name and description data\namazon_train[\"amazon_text\"] = amazon_train[\"name\"]+\" \"+amazon_train[\"description\"]\namazon_test[\"amazon_text\"] = amazon_test[\"name\"]+\" \"+amazon_test[\"description\"]\ngoogle_train[\"google_text\"] = google_train[\"name\"]+\" \"+google_train[\"description\"]\ngoogle_test[\"google_text\"] = google_test[\"name\"]+\" \"+google_test[\"description\"]\n\n# Standaization for numerical data\namazon_train[\"price\"] = num_preprocess(amazon_train[\"price\"])\namazon_test[\"price\"] = num_preprocess(amazon_test[\"price\"])\n\ngoogle_train[\"price\"] = num_preprocess(google_train[\"price\"])\ngoogle_test[\"price\"] = num_preprocess(google_test[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blocking / Indexing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the jaccard distance\ndef jaccard_distance(str1,str2):\n    a = set(str1.split())\n    b = set(str2.split())\n    c = a.intersection(b)\n    jaccard_dist = float(len(c)) / (len(a) + len(b) - len(c))\n    return jaccard_dist\n\n# Get the distance matrix\ndef distance_matrix_generator(amazon_key,google_key):\n    distance_matrix = np.zeros((len(amazon_key),len(google_key)))\n    for i in range(0,len(amazon_key)):\n        for j in range(0,len(google_key)):\n            distance_matrix[i][j] = jaccard_distance(amazon_key[i],google_key[j])\n    return distance_matrix\n\n# Get the potential candidate\ndef potential_matching(amazon_data,google_data,dist_matrix,threshold):\n    candidate_index = np.where(dist_matrix >threshold)\n    # retrieve index for each set\n    amazon_index = candidate_index[0]\n    google_index = candidate_index[1]\n    print(\"length of amazon index: \"+str(len(amazon_index)))\n    print(\"length of google index: \"+str(len(google_index)))\n    # retrieve id for each set\n    amazon_id = (amazon_data[\"idAmazon\"][amazon_index]).tolist()\n    google_id = (google_data[\"idGoogle\"][google_index]).tolist()\n    # calculate the similarity for each pair\n    jaccard_similarity = []\n    for i in range(0,len(amazon_index)):\n         jaccard_similarity.append(round(dist_matrix[amazon_index[i]][google_index[i]],2))\n    # potential candidate\n    potential_pairs = pd.DataFrame({\"idAmazon\":amazon_id,\"idGoogle\":google_id,\"similarity\":jaccard_similarity})\n    return potential_pairs\n\n# Generate the labels\ndef negatives_generator(perfect_matching,potential_matching):\n    # check the quality of blcoking\n    auxiliary = pd.merge(perfect_matching,potential_matching, on=[\"idAmazon\",\"idGoogle\"], how=\"outer\", indicator=True)\n    print(\"true positve/recall: \"+str(len(*np.where(auxiliary[\"_merge\"]==\"both\"))))\n    print(\"false positive/- samples: \"+str(len(*np.where(auxiliary[\"_merge\"]==\"right_only\"))))\n    print(\"false negative/+ lost: \"+str(len(*np.where(auxiliary[\"_merge\"]==\"left_only\")))+\"\\n\")\n    # labelling\n    auxiliary[\"label\"] = np.where(auxiliary[\"_merge\"]==\"both\",1,0) \n    print(\"No. of positives: \"+str(len(*np.where(auxiliary[\"label\"]==1))))\n    print(\"No. of negatives: \"+str(len(*np.where(auxiliary[\"label\"]==0)))+\"\\n\")\n    auxiliary = auxiliary[[\"similarity\",\"idAmazon\",\"idGoogle\",\"label\"]]\n    auxiliary['similarity'].fillna(1,inplace=True) \n    return auxiliary\n\n# Add fields\ndef add_fields(potential_index_labels, amazon_data, google_data):\n    potential_pairs = potential_index_labels\n    amazon_name = []\n    amazon_description = []\n    amazon_info = []\n    amazon_prices = []\n    google_name = []\n    google_description = []\n    google_info = []\n    google_prices = []\n    for i in range(0, len(potential_index_labels)):\n        amazon_name.append((amazon_data.loc[amazon_data[\"idAmazon\"]==potential_index_labels[\"idAmazon\"][i],\"name\"]).item())\n        amazon_description.append((amazon_data.loc[amazon_data[\"idAmazon\"]==potential_index_labels[\"idAmazon\"][i],\"description\"]).item())\n        amazon_info.append((amazon_data.loc[amazon_data[\"idAmazon\"]==potential_index_labels[\"idAmazon\"][i],\"amazon_text\"]).item())\n        amazon_prices.append((amazon_data.loc[amazon_data[\"idAmazon\"]==potential_index_labels[\"idAmazon\"][i],\"price\"]).item())\n        google_name.append((google_data.loc[google_data[\"idGoogle\"]==potential_index_labels[\"idGoogle\"][i],\"name\"]).item())\n        google_description.append((google_data.loc[google_data[\"idGoogle\"]==potential_index_labels[\"idGoogle\"][i],\"description\"]).item())\n        google_info.append((google_data.loc[google_data[\"idGoogle\"]==potential_index_labels[\"idGoogle\"][i],\"google_text\"]).item())\n        google_prices.append((google_data.loc[google_data[\"idGoogle\"]==potential_index_labels[\"idGoogle\"][i],\"price\"]).item())\n    potential_pairs[\"amazon_name\"] = amazon_name\n    potential_pairs[\"google_name\"] = google_name\n    potential_pairs[\"amazon_description\"] = amazon_description\n    potential_pairs[\"google_description\"] = google_description\n    potential_pairs[\"amazon_info\"] = amazon_info\n    potential_pairs[\"google_info\"] = google_info\n    potential_pairs[\"amazon_price\"] = amazon_prices\n    potential_pairs[\"google_price\"] = google_prices\n    potential_pairs[\"price_diff\"] = np.abs(potential_pairs[\"amazon_price\"] - potential_pairs[\"google_price\"])/potential_pairs[[\"amazon_price\",\"google_price\"]].max(axis=1)\n    potential_pairs = potential_pairs[[\"similarity\",\"idAmazon\",\"idGoogle\",\"amazon_name\",\"google_name\",\"amazon_description\",\"google_description\",\"amazon_info\",\"google_info\",\"amazon_price\",\"google_price\", \"price_diff\",\"label\"]]\n    return potential_pairs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# jaccard distance matrix for training sets\ntrain_jaccard_dist = distance_matrix_generator(amazon_train[\"name\"],google_train[\"name\"])\n# jaccard distance matrix for testing sets\ntest_jaccard_dist = distance_matrix_generator(amazon_test[\"name\"],google_test[\"name\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# potential candidates for training sets\ntrain_potential_matching = potential_matching(amazon_train,google_train,train_jaccard_dist,0.1)\nprint(\"No. of potential pairs in training set: \"+str(len(train_potential_matching)))\n# potential candidates for testing sets\ntest_potential_matching = potential_matching(amazon_test,google_test,test_jaccard_dist,0.1)\nprint(\"No. of potential pairs in testing set: \"+str(len(test_potential_matching)))\n# [idAmazon, idGoogle, similarity]\n\nprint(\"\")\n\n# Label\ntrain_index_labels = negatives_generator(train_perfect_matching,train_potential_matching)\n# train_index_labels\ntest_index_labels = negatives_generator(test_perfect_matching,test_potential_matching)\n#[similarity, idAmazon, idGoogle, label]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct the complete training and testing dataset\ntrain_data = add_fields(train_index_labels, amazon_train, google_train)\ntest_data = add_fields(test_index_labels, amazon_test, google_test)\n\n#[similarity, idAmazon, idGoogle, amazon_info, google_info, amazon_price, google_price, price_diff, label]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split val set out of the train set first"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/prepared-data/'\ntrain_data = pd.read_csv(path+'train_data.csv')\ntest_data = pd.read_csv(path+'test_data.csv')","execution_count":114,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Price NAN indicator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def price_nan_indicator(amazon_price, google_price):\n    if amazon_price == 0 or google_price == 0:\n        return 1\n    else:\n        return 0\n    \ntrain_data[\"price_nan_indicator\"] = train_data.apply(lambda x: price_nan_indicator(x.amazon_price, x.google_price), axis=1)\ntest_data[\"price_nan_indicator\"] = test_data.apply(lambda x: price_nan_indicator(x.amazon_price, x.google_price), axis=1)","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_data[[\"similarity\",\"label\",\"amazon_name\",\"google_name\",\"amazon_description\",\"google_description\",\"amazon_info\",\"google_info\",\"price_diff\",\"price_nan_indicator\"]]\ny = train_data[\"label\"]\n\nX, y = shuffle(X, y)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,test_size=0.2,random_state=42)\n\nprint(\"val set: \",len(val_y))\nprint(Counter(val_y))\nprint(\"train set: \",len(train_y))\nprint(Counter(train_y))","execution_count":118,"outputs":[{"output_type":"stream","text":"val set:  12240\nCounter({0: 12039, 1: 201})\ntrain set:  48958\nCounter({0: 48114, 1: 844})\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = pd.concat([train_X, train_y], axis=1)\nval_set = pd.concat([val_X, val_y], axis=1)","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.columns","execution_count":108,"outputs":[{"output_type":"execute_result","execution_count":108,"data":{"text/plain":"Index(['similarity', 'label', 'amazon_name', 'google_name',\n       'amazon_description', 'google_description', 'amazon_info',\n       'google_info', 'price_diff', 'price_nan_indicator', 'label'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set = test_data[['similarity', 'label', 'amazon_name', 'google_name',\n       'amazon_description', 'google_description', 'amazon_info',\n       'google_info', 'price_diff', 'price_nan_indicator']]","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_set.head(1)","execution_count":121,"outputs":[{"output_type":"execute_result","execution_count":121,"data":{"text/plain":"   similarity  label                         amazon_name  \\\n0         1.0      1  clickart premier image pack dvdrom   \n\n                          google_name amazon_description  \\\n0  clickart premier image pack dvdrom                      \n\n                                  google_description  \\\n0  massive collection images fonts design needs o...   \n\n                            amazon_info  \\\n0  clickart premier image pack dvdrom     \n\n                                         google_info  price_diff  \\\n0  clickart premier image pack dvdrom massive col...         1.0   \n\n   price_nan_indicator  \n0                    1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity</th>\n      <th>label</th>\n      <th>amazon_name</th>\n      <th>google_name</th>\n      <th>amazon_description</th>\n      <th>google_description</th>\n      <th>amazon_info</th>\n      <th>google_info</th>\n      <th>price_diff</th>\n      <th>price_nan_indicator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>clickart premier image pack dvdrom</td>\n      <td>clickart premier image pack dvdrom</td>\n      <td></td>\n      <td>massive collection images fonts design needs o...</td>\n      <td>clickart premier image pack dvdrom</td>\n      <td>clickart premier image pack dvdrom massive col...</td>\n      <td>1.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set.head(1)","execution_count":107,"outputs":[{"output_type":"execute_result","execution_count":107,"data":{"text/plain":"       similarity  label                                        amazon_name  \\\n26488        0.12      0  quickbooks premier manufacturer wholesale edition   \n\n                       google_name  \\\n26488  backyard soccer mls edition   \n\n                                      amazon_description  \\\n26488  quickbooks premier manufacturing wholesale edi...   \n\n                                      google_description  \\\n26488  kids get chance play alongside junior versions...   \n\n                                             amazon_info  \\\n26488  quickbooks premier manufacturer wholesale edit...   \n\n                                             google_info  price_diff  \\\n26488  backyard soccer mls edition kids get chance pl...    0.977622   \n\n       price_nan_indicator  label  \n26488                    0      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity</th>\n      <th>label</th>\n      <th>amazon_name</th>\n      <th>google_name</th>\n      <th>amazon_description</th>\n      <th>google_description</th>\n      <th>amazon_info</th>\n      <th>google_info</th>\n      <th>price_diff</th>\n      <th>price_nan_indicator</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26488</th>\n      <td>0.12</td>\n      <td>0</td>\n      <td>quickbooks premier manufacturer wholesale edition</td>\n      <td>backyard soccer mls edition</td>\n      <td>quickbooks premier manufacturing wholesale edi...</td>\n      <td>kids get chance play alongside junior versions...</td>\n      <td>quickbooks premier manufacturer wholesale edit...</td>\n      <td>backyard soccer mls edition kids get chance pl...</td>\n      <td>0.977622</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save train, val, test set\ntrain_X.to_csv(\"train_set.csv\", index = False)\nval_X.to_csv(\"val_set.csv\", index = False)\ntest_set.to_csv(\"test_set.csv\", index = False)","execution_count":123,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# =============================Ignore Below============================="},{"metadata":{"trusted":true},"cell_type":"code","source":"data_amazon_text = train_data[\"amazon_info\"].tolist()\ndata_google_text = train_data[\"google_info\"].tolist()\n# Combine all text data\nall_data_text = data_amazon_text + data_google_text\n\n# Retrieve all the tokens in the dataset\ndef create_tokenizer(all_text):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(all_text)\n    word_index = tokenizer.word_index\n    print(\"Found %s unique tokens\"%len(word_index))\n    return tokenizer\n\n# Create tokenizer according to all the words in train data\ntokenizer = create_tokenizer(all_data_text)\n\ndef tokenization (tokenizer, text, maxlen):\n    #convert to integer lists\n    sequences = tokenizer.texts_to_sequences(text)\n    # padding\n    sequences = pad_sequences(sequences, maxlen)\n    return sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amazon_name = train_data[\"amazon_name\"].tolist()\n# amazon_name_sequence = tokenization(tokenizer,amazon_name, maxlen=200)\n# google_name = train_data[\"google_name\"].tolist()\n# google_name_sequence = tokenization(tokenizer,google_name, maxlen=200)\n# amazon_description = train_data[\"amazon_description\"].tolist()\n# amazon_desc_sequence =  tokenization(tokenizer,amazon_description, maxlen=200)\n# google_description = train_data[\"google_description\"].tolist()\n# google_desc_sequence = tokenization(tokenizer,google_description, maxlen=200)\n# amazon_info = train_data[\"amazon_info\"].tolist()\n# amazon_info_sequence = tokenization(tokenizer,amazon_info, maxlen=200)\n# google_info = train_data[\"google_info\"].tolist()\n# google_info_sequence = tokenization(tokenizer,google_info, maxlen=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data = train_data.drop(\"amazon_name_seq\", axis=1)\n# train_data = train_data.drop(\"google_name_seq\", axis=1)\n# train_data = train_data.drop(\"amazon_desc_seq\", axis=1)\n# train_data = train_data.drop(\"google_desc_seq\", axis=1)\n# train_data = train_data.drop(\"amazon_info_seq\", axis=1)\n# train_data = train_data.drop(\"google_info_seq\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# amazon_name_list = []\n# amazon_desc_list = []\n# amazon_info_list = []\n# google_name_list = []\n# google_desc_list = []\n# google_info_list = []\n# for i in range(0,len(train_data)):\n#     amazon_name_list.append(amazon_name_sequence[i].tolist())\n#     amazon_desc_list.append(amazon_desc_sequence[i].tolist())\n#     amazon_info_list.append(amazon_info_sequence[i].tolist())\n#     google_name_list.append(google_name_sequence[i].tolist())\n#     google_desc_list.append(google_desc_sequence[i].tolist())\n#     google_info_list.append(google_info_sequence[i].tolist())\n    \n# amazon_name_list \n# train_data[\"amazon_name_seq\"] = amazon_name_list\n# train_data[\"google_name_seq\"] = google_name_list\n# train_data[\"amazon_desc_seq\"] = amazon_desc_list\n# train_data[\"google_desc_seq\"] = google_desc_list\n# train_data[\"amazon_info_seq\"] = amazon_info_list\n# train_data[\"google_info_seq\"] = google_info_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data[\"amazon_name_seq\"] = train_data[\"amazon_name_seq\"].apply(lambda x: np.asarray(x))\n# train_data[\"google_name_seq\"] = train_data[\"google_name_seq\"].apply(lambda x: np.asarray(x))\n# train_data[\"amazon_desc_seq\"] = train_data[\"amazon_desc_seq\"].apply(lambda x: np.asarray(x))\n# train_data[\"google_desc_seq\"] = train_data[\"google_desc_seq\"].apply(lambda x: np.asarray(x))\n# train_data[\"amazon_info_seq\"] = train_data[\"amazon_info_seq\"].apply(lambda x: np.asarray(x))\n# train_data[\"google_info_seq\"] = train_data[\"google_info_seq\"].apply(lambda x: np.asarray(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data[\"amazon_name_seq\"] = amazon_name_sequence.tolist()\n# train_data[\"amazon_desc_seq\"] = amazon_desc_sequence.tolist()\n# train_data[\"google_name_seq\"] = google_name_sequence.tolist()\n# train_data[\"google_desc_seq\"] = google_desc_sequence.tolist()\n# train_data[\"amazon_info_seq\"] = amazon_info_sequence.tolist()\n# train_data[\"amazon_info_seq\"] = amazon_info_sequence.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_data.to_csv(\"train_data_full.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_amazon_name = test_data[\"amazon_name\"].tolist()\n# test_amazon_name_sequence = tokenization(tokenizer,test_amazon_name, maxlen=200)\n# test_google_name = test_data[\"google_name\"].tolist()\n# test_google_name_sequence = tokenization(tokenizer,test_google_name, maxlen=200)\n# test_amazon_description = test_data[\"amazon_description\"].tolist()\n# test_amazon_desc_sequence =  tokenization(tokenizer,test_amazon_description, maxlen=200)\n# test_google_description = test_data[\"google_description\"].tolist()\n# test_google_desc_sequence = tokenization(tokenizer,test_google_description, maxlen=200)\n# test_amazon_info = test_data[\"amazon_info\"].tolist()\n# test_amazon_info_sequence = tokenization(tokenizer,test_amazon_info, maxlen=200)\n# test_google_info = test_data[\"google_info\"].tolist()\n# test_google_info_sequence = tokenization(tokenizer,test_google_info, maxlen=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_data[\"amazon_name_seq\"] = test_amazon_name_sequence.tolist()\n# test_data[\"amazon_desc_seq\"] = test_amazon_desc_sequence.tolist()\n# test_data[\"google_name_seq\"] = test_google_name_sequence.tolist()\n# test_data[\"google_desc_seq\"] = test_google_desc_sequence.tolist()\n# test_data[\"amazon_info_seq\"] = test_amazon_info_sequence.tolist()\n# test_data[\"amazon_info_seq\"] = test_amazon_info_sequence.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.shape)\ntest_data.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.to_csv(\"test_data_full.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}